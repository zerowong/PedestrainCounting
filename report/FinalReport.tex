\documentclass[a4paper]{article}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage[colorlinks, linkcolor=]{hyperref}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{subfigure}

\lstset{language=C++,
        basicstyle=\ttfamily,
        keywordstyle=\color{blue}\ttfamily,
        stringstyle=\color{red}\ttfamily,
        commentstyle=\color{green}\ttfamily,
        frame=single,
        numbers=left,
        morecomment=[l][\color{magenta}]{\#}
}

\title{Final Report of Pedestrain Counting}
\author{Wang Zhengrong, Hsienyu Meng, Liuyang Zhan}

\begin{document}
\maketitle
\begin{enumerate}
\item \textbf{Introduction}

Pedestrain detection and counting has been heavily researched in the last few years and people have made great improvement. Dalal's HoG (Histogram of Gradients) operator has been widely used in pedestrain detection with SVM, AdaBoost or other machine learning algorithms, which we use as the basic detector in our project.

However, detection is not enough to count the pedestrains in the ROI, therefore we have to track each person. Our basic idea is to use particle filter to track each specific person.

Particle filter is an approximation of Bayes inference and is widely used in tracking. Compared with Karman filter, it can simulate any probability distribution. However it's main drawback is the high complexity of computation. Which we will try to optimize with multiple threads.

This project is hosted as a private project on GitHub. You will find the \href{https://zerowong.github.io/PedestrainCounting}{project page} and the \href{https://zerowong.github.io/PedestrainCounting/docs/html/index.html}{documents} on it.

\item \textbf{Basic Plan}

Here is our basic plan for this project.

\begin{itemize}
\item Code Reconstruction

The code offered by the teacher is not object-oriented, and is very difficult to modify and extend. Hence our first goal is to reconstruct the program so that we can easily build our particle filter on it.

\item Merge Paritcle Filter

The main idea is from \cite{eth_biwi_00633}, in which there are mainly two new ideas. The first one is that instead of using one offline trained general classifier, they train one online classifier for each detected pedestrain and the classifier is only updated on non-overlapping detections. Secondly, the detections are used to guide the particles' propagation which is implemented to estimates the conditional likelihood of the new observation .

\begin{itemize}

\item Data Association Problem
Use the greedy algorithm to find the $pair(t_r,d)$ with maximum score in the matching score matrix and delete the columns and rows belonging to tracker $t_r^*$ and $d$ 

\item Online Boosting

% Select feature
The online boosting classifier for each pedestrain is similar to that in \cite{Grabner:2006:OBV:1153170.1153451} and we will select some features to train it.

\end{itemize}

\item Optimization

With multiple threads or even GPU programming, we may archieve the real time interactive result.

\end{itemize}

\item \textbf{Code Reconstruction}

First we reconstruct the code. We left kmeas and meanshift algorithm unchanged cause they are not important in our project. And we divide the whole project into these 5 parts.

\begin{itemize}

\item Utility

We implement some utility classes here. Mainly some geometry classes such as \lstinline{Size}, \lstinline{Rect}, \lstinline{Point2D}. These are very similar to those in OpenCV library. However we still implement them as sometimes we need overload some operators. We also implement a container called \lstinline{Pool}, which is basically just a vector that never shrinks, in order to improve performance.

And we also reconstruct the \lstinline{ConnectedComponents} here. It basically does the same thing as before.

\item IntegralImage

As most of the features will be extracted using integral image to speed up, we implement an \lstinline{IntegralImage} interface. This is an abstract class containing some virtual functions. The most important method is:

\begin{lstlisting}[language=C++]
// Normal integral image.
virtual unsigned int GetSum(const Rect &roi) const;

// Used in HoG integral image.
virtual void GetSum(const Rect &roi, float *result) const;
\end{lstlisting}

Other integral image classes should overload these two functions according to their purpose. Here we mainly implement 3 integral images.

\lstinline{GrayScaleIntegralImage} calculates the integral image for a grayscale image. It overloads the first \lstinline{GetSum} function.

\lstinline{HoGIntegralImage} calculates the 9 bins HoG for a grayscale image. Of course this is used to extract the HoG feature.

\lstinline{RGIIntegralImage} calculates the integral image for 3 channel: RED, BLUE, INTENSITY.

\item FeatureExtractor

In this part we implement three classes: \lstinline{Feature}, \lstinline{HaarFeature}, \lstinline{HoGFeature}, \lstinline{RGIFeature}.

\lstinline{Feature} is bascially just a container for the feature we extraced using the other two classes.

\lstinline{HaarFeature} extracts a haar-like feature given an integral image and roi. When being constructed, it randomly chooses from the five haar-like features.

\lstinline{HoGFeature} extracts a HoG feature given an \lstinline{HoGIntegralImage} and roi.

\lstinline{RGIFeature} extracts a RGI feature given an \lstinline{RGIIntegralImage} and roi.

\item Classifier

Here we reconstruct the original AdaBoost classifier with the following classes.

First we build an \lstinline{WeakClassifier} interface and it has two main virtual methods:

\begin{lstlisting}
virtual bool Update(const IntegralImage *intImage, 
	const Rect &roi, int target);
virtual float Evaluate(const IntegralImage *intImage, 
	const Rect &roi);
\end{lstlisting}

\lstinline{Evaluate} evaluates the roi with the feature inside this weakclassifier, while \lstinline{Update} is used in training.

Then we implement a class called \lstinline{WeakClassifierHoG}. It doesn't overload \lstinline{Update} method therefore it can't be trained. It's only used in the offline AdaBoost classifier.

We construct the AdaBoost classifier using \lstinline{WeakClassifierHoG}.

\item Detector

With the AdaBoost classifier above we are able to build the detectors now.

\lstinline{ImageDetector} uses the AdaBoost classifier and slide windows to detect pedestrain in the whole image.

\lstinline{BKGCutDetector} inherits from \lstinline{ImageDetector}. It cuts the background and uses the \lstinline{ConnectedComponents} to speed up the detection. When it is not sure whether a connected component is a pedestrain or not, it calls \lstinline{ImageDetector} to judge.

\lstinline{VideoDetector} receives a pointer of \lstinline{ImageDetector} and use it to detect pedestrain in every two frames. Notice that with virtual function we can use \lstinline{BKGCutDetector} here as well.

\end{itemize}

Besides, while reconstructing the program, we rewrite some parts of the program in a more memory friendly way, which leads to quite tremendous improvement. The original video detector on the first training video takes 212s, while our reconstructed program takes 66s with one main thread. After optimizing some parameters it reduces to 27s without deteriorating its precision.

Here are some results from our reconstruction: Figure~\ref{fig:detector}. We can see that with background cut we have less false positive.
\begin{figure}[htb]
    \begin{center}
        \subfigure[ImageDetector]{\label{fig:sub:ImageDetector}
            \begin{minipage}[c]{0.5\textwidth}
            \centering
            \includegraphics[width = \textwidth]{images/ImageDetector.jpg}
            \end{minipage}%
        }%
        \subfigure[BKGCutDetector]{\label{fig:sub:BKGCutDetector}
            \begin{minipage}[c]{0.5\textwidth}
            \centering
            \includegraphics[width = \textwidth]{images/BKGCutDetector.jpg}
            \end{minipage}%
        }%
    \end{center}
    \caption{Detection Results}
    \label{fig:detector}
\end{figure}

\item \textbf{Online Boosting}

After the reconstruction, we start to work on the online boosting algorithm to track a single target. The main work is focus on Classifier part. We implement the following new classes.

\begin{itemize}

\item \lstinline{EstimatedGaussianDistribution}

Given a feature $f(\mathbf{x})$, the probability of $P(1|f(\mathbf{x}))$ and $P(-1|f(\mathbf{x}))$ is estimated as Gaussian distribution\cite{Grabner:2006:OBV:1153170.1153451}. This Gaussian distribution is estimated with Kalman filter\cite{Welch:1995:IKF:897831}. We use the following update equations for adaptive estimation from \cite{Grabner:2006:OBV:1153170.1153451}:

\begin{subequations}
\begin{align}
K_{t}&=\frac{P_{t-1}}{P_{t-1}+R}\\
\mathbf{\mu}_{t}&=K_{t}f(\mathbf{x})+(1-K_{t})\mathbf{\mu}_{t-1}\\
\mathbf{\sigma}_{t}^{2}&=K_{t}(f(\mathbf{x})-\mathbf{\mu}_{t})^{2}+(1-K_{t})\mathbf{\sigma}_{t-1}^{2}\\
P_{t}&=(1-K_{t})P_{t-1}
\end{align}
\end{subequations}

\item \lstinline{ClassifierThreshold}

It estimates the Gaussian distribution for both positive features $N(\mathbf{\mu_{+}}, \mathbf{\sigma_{+}})$ and negative features $N(\mathbf{\mu_{-}}, \mathbf{\sigma_{-}})$. Then it uses a simple distance threshold to a new feature to whether positive or negative: $h(\mathbf{x})$ for "hypothesis"

\begin{equation}
h(\mathbf{x})=\min_{+, -}(D(f(\mathbf{x}), \mu_{+}), D(f(\mathbf{x}), \mu_{-}))
\end{equation}

where $D(f(\mathbf{x}), \mu)$ is just the Euclidean distance in feature space.

\item \lstinline{WeakClassifier}

It uses some feature above and the \lstinline{ClassifierThreshold} to build a weak classifier. For classify, it uses \lstinline{HaarFeature} to extract the feature and sends it to \lstinline{ClassifierThreshold} to classify. For training, it uses the Kalman filter in \lstinline{EstimatedGaussianDistribution}.

Here we will test Haar-like feature and RGI feature for online boosting.

\item \lstinline{ClassifierSelector}

Given a pool of weak classifiers, the \lstinline{ClassifierSelector} selects the best one with lowest error rate.

\begin{itemize}

\item Training

Each training feature $f(\mathbf{x})$ has an importance $\lambda$, and we use the idea from \cite{Oza01onlinebagging} to draw a random variable $k\sim Poisson(\lambda)$ and this feature is trained for $k$ times.

\item Selecting

For each weak classifier, we maintain two vaiables $\lambda_{correct}$ and $\lambda_{wrong}$:

\begin{subequations}
\begin{align}
\lambda_{correct}&=\sum_{i_{correct}}\lambda_{i}\\
\lambda_{wrong}&=\sum_{i_{wrong}}\lambda_{i}
\end{align}
\end{subequations}

And the error rate is estimated by:

\begin{equation}
err=\frac{\lambda_{wrong}}{\lambda_{correct}+\lambda_{wrong}}
\end{equation}

Then we choose the best weak classifier with lowest error rate.

\item Replacing

To improve the performance, each time we not only choose the best weak classifier but also replace the worst one with a randomly generated new weak classifier.

\end{itemize}

\item \lstinline{StrongClassifier}

The \lstinline{StrongClassifier} has $N$ \lstinline{ClassifierSelector}s, each with a voting weight $\alpha_i$. The final hypothesis is:

\begin{equation}
h^{strong}(\mathbf{x})=\mathrm{sign}(\sum_{i=1}^{N}\alpha_{i}\cdot h^{selector}_{i}(\mathbf{x}))
\end{equation} 

Suppose $err_i$ is the error rate of the $i^{th}$ selector, and then the voting weight is:

\begin{equation}
\alpha_i=\ln(\frac{1-err_i}{err_i})
\end{equation}

And the importance of this sample is updated with:

\begin{subequations}
\begin{align}
\lambda_{i+1}&=\lambda_i\cdot\sqrt{\frac{err_i}{1-err_i}}, &if~h^{selector}_{i}~correct\\
\lambda_{i+1}&=\lambda_i\cdot\sqrt{\frac{1-err_i}{err_i}}, &if~h^{selector}_{i}~wrong
\end{align}
\end{subequations}
\end{itemize}

\item \textbf{Particle Filter}

After we have the online boosting strong classifier. We try to combine it with particle filter.

\begin{itemize}

\item \lstinline{SingleSampler}

Given a target, it samples around it the positive and negative samples using Gaussian noise. Here is the samples: Figure~\ref{fig:singlesampler}, red for negative samples and blue for positive ones.

This is used in training the classifier.

\begin{figure}[htb]
    \begin{center}
        \subfigure[SingleSampler]{\label{fig:sub:singlesampler}
            \begin{minipage}[c]{0.5\textwidth}
            \centering
            \includegraphics[width = \textwidth]{images/SingleSampler.jpg}
            \end{minipage}%
        }%
    \end{center}
    \caption{Detection Results}
    \label{fig:singlesampler}
\end{figure}

\item \lstinline{ParticleFilterConstVelocity}

This is a basic particle filter with constant velocity. The state space is just the position $[upper, left]$ and the velocity. The motion model is also very simple:

\begin{align}
p_{t}&=p_{t-1}+v_{t-1}+N(0, \sigma_p)\\
v_{t}&=v_{t-1}+N(0, \sigma_v)
\end{align}

where $N(0, \sigma_p)$ is a Gaussian random variable with variance proportional to the size of the target, and $N(0, \sigma_v)$ is a Gaussian random vairiable with variance proportional to how many frames in the past has been successfully detected.

\item \lstinline{ParticleFilterTracker}

This class just combines everything together, use particle filter and strong classifier to track a target.

\end{itemize}

\item \textbf{Match Matrix}

Since we are trying to track mulitple targets, we will have multiple detections and targets in one frame. In order to solve this data-association problem, we define a match score\cite{eth_biwi_00633}:

\begin{equation}
s(tr, d)=g(tr, d)\cdot(c_{tr}(d)+\alpha\cdot\sum_{p\in tr}^{N}p_{N}(d-p))
\end{equation}

The exact meaning of this equation can be found in \cite{eth_biwi_00633}. We won't talk much about it here.

A match score matrix will be calculated for each pair of (target, detection). Then a greedy algorithm will be applied to find the match.

\begin{enumerate}
\item find the maximum match score and set this as a matched pair
\item eliminate this target and this detection
\item go back to (a) if there are still unmatched detections and targets
\item finally only pairs with match score higher than a threshold will be taken
\end{enumerate}

\item \textbf{Experiment}

All the experiment is executed on Dell Inspiron 14R SE with the following configuration.
All the experiment result can be found \href{https://zerowong.github.io/PedestrainCounting/results/}{here}.

\begin{itemize}

\item Intel Core i7-3612QM 2.10GHz
\item 8.00GB RAM
\item System: Windows 8.1 x64
\item OpenCV: 3.00

\end{itemize}

For video we have three clips: \href{https://zerowong.github.io/PedestrainCounting/results/test/1.avi}{1.avi}, \href{https://zerowong.github.io/PedestrainCounting/results/test/2.avi}{2.avi}, \href{https://zerowong.github.io/PedestrainCounting/results/test/3.avi}{3.avi}.

For single image detection we only use \href{https://zerowong.github.io/PedestrainCounting/results/test/test.jpg}{test.jpg}, which is from \href{https://zerowong.github.io/PedestrainCounting/results/test/1.avi}{1.avi}.

The original baseline program is tested with all default settings.

With the following configuration we tested our single tracker on a small video clip. The result can be found \href{https://zerowong.github.io/PedestrainCounting/results/singleTrackerTestGray01.avi}{here}.

\begin{itemize}

\item The online boosting classifier is built with RGI subpatch feature.
\item The particle filter resamples with confidence of each particle.
\item 500 particles.
\item User initializes the target bounding box.

We can see that the classifier works quite good in Figure~\ref{fig:particlesConfidence}. After observation, the particles are drawn with their confidence. The brighter the particle is, the more confidence it has. Notice that the particle is at the upper-left corner of the target.

\begin{figure}[htb]
    \begin{center}
        \subfigure[]{\label{fig:sub:particlesConfidence}
            \begin{minipage}[c]{0.5\textwidth}
            \centering
            \includegraphics[width = \textwidth]{images/ParticlesConfidence.jpg}
            \end{minipage}%
        }%
    \end{center}
    \caption{Particles Confidence}
    \label{fig:particlesConfidence}
\end{figure}

\item Drift problem.

From Figure~\ref{fig:singleTrackerGrayExchange} we can see that the current grayscale Haar-like classifier can still not distinguish different pedestrains. We think this is due to the grayscale haar-like feature doesn't contain enough information. \cite{eth_biwi_00633} reports that with RGI (Red, Green, Intensity) histogram feature, 3 bins for each channel the results are quite good. 

Hence we use RGI feature to construct the weak classifier. The resulting video can be found \href{https://zerowong.github.io/PedestrainCounting/results/singleTrackerTestRGI01.avi}{here}. From this video and Figure~\ref{fig:singleTrackerRGIExchange} we can see that the classifier is more robustic and can handle some situation as two people crossing each other. However in Figure~\ref{fig:sub:singleRGI02} the classifier still goes for wrong pedestrain! Well they wear the same jeans.

\begin{figure}[htb]
    \begin{center}
        \subfigure[Original target]{\label{fig:sub:singleGray01}
            \begin{minipage}[c]{0.5\textwidth}
            \centering
            \includegraphics[width = \textwidth]{images/SingleTrackerGrayExchange01.jpg}
            \end{minipage}%
        }%
        \subfigure[Drift to another target]{\label{fig:sub:singleGray02}
            \begin{minipage}[c]{0.5\textwidth}
            \centering
            \includegraphics[width = \textwidth]{images/SingleTrackerGrayExchange02.jpg}
            \end{minipage}%
        }%
    \end{center}
    \caption{With grayscale haar-like feature, the tracker lost its target.(Black for original target)}
    \label{fig:singleTrackerGrayExchange}
\end{figure}

\begin{figure}[htb]
    \begin{center}
        \subfigure[Original target]{\label{fig:sub:singleRGI01}
            \begin{minipage}[c]{0.5\textwidth}
            \centering
            \includegraphics[width = \textwidth]{images/SingleTrackerRGIExchange01.jpg}
            \end{minipage}%
        }%
        \subfigure[Drift to another target]{\label{fig:sub:singleRGI02}
            \begin{minipage}[c]{0.5\textwidth}
            \centering
            \includegraphics[width = \textwidth]{images/SingleTrackerRGIExchange02.jpg}
            \end{minipage}%
        }%
    \end{center}
    \caption{With RGI feature, the tracker is more robustic, but still lost its target finally. (Black for original target)}
    \label{fig:singleTrackerRGIExchange}
\end{figure}

\item HoG Detector.

We first use the simple HoG detector to detect a single picture.
\begin{center}
\begin{tabular}{l | c | c | r}
\hline
                & Time  & Accuracy  & False Positive \\
Original        & 3.43s & 5 / 6     & 3              \\
Reconstructed   & 3.20s & 5 / 6     & 2              \\
\hline
\end{tabular}
\end{center}

\item Background Cut Detector.

We use the simple Background cut detector to detect pedestrain in a single picture.

\begin{center}
\begin{tabular}{l | c | c | r}
\hline
                & Time  & Accuracy  & False Positive \\
Original        & 1.76s & 6 / 6     & 0              \\
Reconstructed   & 1.27s & 5 / 6     & 0              \\
\hline
\end{tabular}
\end{center}

\item HoG Detector for Video.

We use the simple HoG detector to detect pedestrain in a video.

\begin{center}
\begin{tabular}{l | c | c | r}
\hline
                & Time  & Accuracy  & False Positive \\
Original        & 1051s  &           &                \\
Reconstructed   & 900s &           &                \\
\hline
\end{tabular}
\end{center}

\item Background Cut Detector for Video.

We use the simple Background Cut detector to detect pedestrain in a video.

\begin{center}
\begin{tabular}{l | c | c | r}
\hline
                & Time  & Accuracy  & False Positive \\
Original        & 207.36s  &           &                \\
Reconstructed   & 159.14s  &           &                \\
\hline
\end{tabular}
\end{center}

\item Multiple Pedestrains Tracking and Counting

This is the finally object of this project. However after two months' hard-working, we still can't get satisfactory result. The result can be found \href{https://zerowong.github.io/PedestrainCounting/results/multiTargetTracking01.avi}{here}. We will analyse the results.

First we can see in Figure~\ref{fig:sub:multiTrackerInit} that when there are no occlution, the detector works fine, and the multiple tracker successfully initializes the three targets and the count them.

However when the pedestrain is occluded, the detector failed to detect him. And since we didn't have any occlusion reasoning for this situation, the particle filter soon lost its target, which can been seen in Figure~\ref{fig:sub:multiTrackerLost}.

\begin{figure}[htb]
    \begin{center}
        \subfigure[]{\label{fig:sub:multiTrackerInit}
            \begin{minipage}[c]{0.5\textwidth}
            \centering
            \includegraphics[width = \textwidth]{images/MultiTrackerInit01.png}
            \end{minipage}%
        }%
        \subfigure[]{\label{fig:sub:multiTrackerLost}
            \begin{minipage}[c]{0.5\textwidth}
            \centering
            \includegraphics[width = \textwidth]{images/MultiTrackerLost01.png}
            \end{minipage}%
        }%
    \end{center}
    \caption{Initialization of Multiple Tracker}
    \label{fig:multiTracker}
\end{figure}

\end{itemize}

\item \textbf{Analyzation}

\begin{itemize}
\item Accomplishment

We reconstructed the original baseline Adaboost classifier and improved the performance.

An online-boosting tracking system has been successfully implemented. It is quite good in tracking for single target. We also tried HoG, Haar-like and RGI features.

\item Problems

Since this is a tracking system based on detection, it requires a state of art detector. However the detector we used here is not so good. If the detector fails to detect the target for consecutive frames, the particle filter will also loses its target.

What's more, the match matrix is not working very fine. If there are multiple targets nearby, it sometimes gets confused.

All of these problems limit the performance of our system.

\item Solutions

The solutions to the above problem is straight forward. Use a state-of-art detector will significantly improve the performance.

\end{itemize}

\item \textbf{Continous Energy Minimize Method}


\end{enumerate}

\clearpage
\bibliographystyle{plain}
\bibliography{Ref}

\end{document}
